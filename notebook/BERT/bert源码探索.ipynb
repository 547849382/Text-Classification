{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要使用tensorflow1.12以上的版本\n",
    "\n",
    "且不能使用tensorflow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\GitHub\\\\Text-Classification'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'下列关于“克隆羊”、“试管羊”、“转基因羊”的说法合理的是（）A它们的生殖方式相同B它们的遗传物质都只来自于一个亲本C它们的形成过程操作水平相同D在培育过程中都要运用胚胎移植技术题型:单选题|难度:一般|使用次数:0|答案：D解析：【解答】解：A、克隆动物采用了核移植技术，属于无性生殖，试管动物主要采用了体外受精技术，属于有性生殖，转基因动物是通过导入外源基因的方式来获取亲本的优良性状，A错误；B、克隆动物遗传物质分别来自提供细胞核和卵细胞质的亲本，试管动物的遗传物质来自有性生殖的两个亲本，转基因动物遗传物质来自有性生殖的两个亲本和提供目的基因的个体，可见，三种动物的遗传物质都不是只来自于一个亲本，B错误；C、“克隆羊”主要采用了核移植技术，属于细胞水平上的操作；“试管羊”主要采用的体外受精技术，属于胚胎水平上的操作；“转基因羊”主要采用了基因工程技术，属于分子水平上的操作，C错误；D、这三者在培育过程中都要运用胚胎移植技术，D正确．据此可知答案为：D．【考点精析】根据题目的已知条件，利用动物细胞核移植技术和胚胎移植的相关知识可以得到问题的答案，需要掌握动物体细胞核移植是将动物的一个细胞的细胞核，移入一个已经去掉细胞核的卵母细胞中，使其重组并发育成一个新的胚胎，这个新胚胎最终发育为动物个体；胚胎移植是将雌性动物的早期胚胎移植到同种的、生理状态相同的其他雌性动物的体内，使之继续发育为新个体的技术．'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0,'content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存label\n",
    "label = set(' '.join(df['label']).strip().split())\n",
    "\n",
    "with open('data/label.txt', 'w', encoding='utf-8') as f:\n",
    "    for l in label:\n",
    "        f.write(l+'\\n')\n",
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取label\n",
    "label = []\n",
    "with open('data/label.txt', 'r', encoding='utf-8') as f:\n",
    "    label = [l for l in f.read().strip().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['宇宙中的地球', '地球运动的基本形式', '地球的内部圈层结构及特点', '社会主义是中国人民的历史性选择', '减数分裂的概念', '基因的分离规律的实质及应用', '组成细胞的化合物', '基因工程的概念', '人体水盐平衡调节', '拉马克的进化学说']\n"
     ]
    }
   ],
   "source": [
    "print(label[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入实例\n",
    "\n",
    "一个InputExample实例如字面意思，包含了4个属性。\n",
    "\n",
    "guid 是表示这格实例属于训练集、验证集或测试集的第几个样本：如`'train-34'`, `'dev-28'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "  def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "    \"\"\"Constructs a InputExample.\n",
    "\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "    self.guid = guid\n",
    "    self.text_a = text_a\n",
    "    self.text_b = text_b\n",
    "    self.label = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理基类\n",
    "\n",
    "read_tsv 就是以\\t分割的数据。在保存数据的时候指定一下分割符即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "  \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "  def get_train_examples(self, data_dir):\n",
    "    \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_test_examples(self, data_dir):\n",
    "    \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @classmethod\n",
    "  def _read_tsv(cls, input_file, quotechar=None):\n",
    "    \"\"\"Reads a tab separated value file.\"\"\"\n",
    "    with tf.gfile.Open(input_file, \"r\") as f:\n",
    "      reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "      lines = []\n",
    "      for line in reader:\n",
    "        lines.append(line)\n",
    "      return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaiduProcessor(DataProcessor):\n",
    "  \"\"\"Processor for the CoLA data set (GLUE version).\"\"\"\n",
    "\n",
    "  def get_train_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "  def get_test_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    label_file = 'data/label.txt'  # 标签存放的位置\n",
    "    labels = [l.strip() for l in open(label_file, encoding='utf-8').readlines()]\n",
    "    return labels\n",
    "\n",
    "  def _create_examples(self, lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "      if i == 0:\n",
    "        continue\n",
    "      guid = \"%s-%s\" % (set_type, i)\n",
    "      text_a = tokenization.convert_to_unicode(line[1])\n",
    "      text_b = None\n",
    "      if set_type == \"test\":\n",
    "        label = \"0\"\n",
    "      else:\n",
    "        label = tokenization.convert_to_unicode(line[0])\n",
    "      examples.append(\n",
    "          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 我的数据处理类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "processer = BaiduProcessor()\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('政治 科学社会主义常识 社会主义是中国人民的历史性选择',\n",
       " 'dev-1',\n",
       " '鱼传尺素、鸿雁传书、目断鲜鸿，这样的文化传统代代相因、世世相袭，融入百姓生活，升华为中华民族独特的文化现象一家书文化。据记者了解，2017年央视春晚将坚持“三不用原则”，即：低俗媚俗的节目不用，格调不高的节目不用，有污点和道德瑕疵的演员不用。坚持“三不用原则”，符合社会主义核心价值观的建设要求。之所以这样做是因为（）①优秀文化才对人具有潜移默化和深远持久的影响②必须加强对文化市场的管理和引导③广播电视成为低俗文化传播的主要手段④落后、腐朽文化严重腐蚀人们的精神世界A①②B①③C③④D②④题型:单选题|难度:一般|使用次数:0|答案：D解析：坚持“三不用原则”，即：低俗媚俗的节目不用，格调不高的节目不用，有污点和道德瑕疵的演员不用。坚持“三不用原则”，符合社会主义核心价值观的建设要求。之所以这样做是因为落后、腐朽文化严重腐蚀人们的精神世界，国家要加强管理、正确引导，②④适合题意；①说法正确，但不是本题的原因，排除；大众传媒是文化传播的主要手段，排除③。据此可知答案为：D。')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = processer.get_dev_examples(data_dir)\n",
    "x[0].label, x[0].guid, x[0].text_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = processer.get_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理部分\n",
    "## convert_single_example\n",
    "\n",
    "实现的过程是使：InputExample -> InputFeatures\n",
    "\n",
    "input_ids : Token Embeddings<br>\n",
    "segment_ids : Segment Embeddings<br>\n",
    "label_id : 标签的类别<br>\n",
    "- 如果是2分类或多分类，是int，如0 or 1 or 2 or .. \n",
    "- 如果是多分类，为列表如：`[0, 0, 1, 1, 0, 1]`\n",
    "\n",
    "input_mask : 如果设定的长度比句子长度长，多出来的部分就要被mask掉。就是padding部分要被mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../data/img/w13_bert_input.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids,\n",
    "                 label_id,\n",
    "                 is_real_example=True):\n",
    "        self.input_ids = input_ids  # \n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids # \n",
    "        self.label_id = label_id\n",
    "        self.is_real_example = is_real_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "\n",
    "    When running eval/predict on the TPU, we need to pad the number of examples\n",
    "    to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "    size. The alternative is to drop the last batch, which is bad because it means\n",
    "    the entire output data won't be generated.\n",
    "\n",
    "    We use this class instead of `None` because treating `None` as padding\n",
    "    battches could cause silent errors.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
    "                                                     tokenizer):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        return InputFeatures(\n",
    "                input_ids=[0] * max_seq_length,\n",
    "                input_mask=[0] * max_seq_length,\n",
    "                segment_ids=[0] * max_seq_length,\n",
    "                label_id=0,\n",
    "                is_real_example=False)\n",
    "\n",
    "    label_map = {}\n",
    "    for (i, label) in enumerate(label_list):\n",
    "        label_map[label] = i\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    if tokens_b:\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0     0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    if tokens_b:\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    label_id = example_label2id(example.label, label_map)  # 修改部分\n",
    "    if ex_index < 5:\n",
    "        tf.logging.info(\"*** Example ***\")\n",
    "        tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "        tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "                [tokenization.printable_text(x) for x in tokens]))\n",
    "        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        # tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))  # 修改部分\n",
    "        tf.logging.info(\"label_id: %s\" % \" \".join([str(x) for x in label_id]))  # 修改部分\n",
    "\n",
    "    feature = InputFeatures(\n",
    "            input_ids=input_ids,\n",
    "            input_mask=input_mask,\n",
    "            segment_ids=segment_ids,\n",
    "            label_id=label_id,\n",
    "            is_real_example=True)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _truncate_seq_pair\n",
    "\n",
    "长句子截断，是tokens_a和b同时截断。且尽量保证两个句子一样长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['你', '好', '世', '界', '！'], ['世', '界', '说', '：', '我', '不', '好', '。'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_a = ['你','好','世','界','！']\n",
    "tokens_b = ['世', '界', '说', '：', '我', '不', '好', '。']\n",
    "_truncate_seq_pair(tokens_a, tokens_b, 15)\n",
    "tokens_a, tokens_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['你', '好', '世', '界', '！'], ['世', '界', '说', '：'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_a = ['你','好','世','界','！']\n",
    "tokens_b = ['世', '界', '说', '：', '我', '不', '好', '。']\n",
    "_truncate_seq_pair(tokens_a, tokens_b, 9)\n",
    "tokens_a, tokens_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改label_id的生成方式\n",
    "\n",
    "由于是多标签分类，需要修改label_id的生成方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def example_label2id(label, label_map):\n",
    "    \"\"\" 将样本标签转为数字 \"\"\"\n",
    "    n_class = len(label_map)  # 多标签的类别数\n",
    "    label_id = [0] * n_class  # 初始化label_id为全0\n",
    "    for lb in label.split():  # 根据样本的label，把对应位置标签置1\n",
    "        label_id[label_map[lb]] = 1\n",
    "    return label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'政治 科学社会主义常识 社会主义是中国人民的历史性选择'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(labels):\n",
    "    label_map[label] = i\n",
    "x[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(example_label2id(x[0].label, label_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\GitHub\\Text-Classification\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex_index = 1\n",
    "example = x[2]\n",
    "max_seq_length = 50\n",
    "vocab_file = 'bert/pretrained_model/chinese_L-12_H-768_A-12/vocab.txt'\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-3\n",
      "INFO:tensorflow:tokens: [CLS] 噬 菌 体 、 细 菌 和 酵 母 菌 都 具 有 的 物 质 或 结 构 是 （ ） [UNK] . 细 胞 壁 [UNK] . 细 胞 质 [UNK] . 细 胞 膜 [UNK] . 遗 传 物 质 题 型 : 单 选 [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1693 5826 860 510 5301 5826 1469 6997 3678 5826 6963 1072 3300 4638 4289 6574 2772 5310 3354 3221 8020 8021 100 119 5301 5528 1880 100 119 5301 5528 6574 100 119 5301 5528 5606 100 119 6890 837 4289 6574 7579 1798 131 1296 6848 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_id: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n"
     ]
    }
   ],
   "source": [
    "f = convert_single_example(ex_index, example, labels, max_seq_length, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## file_based_convert_examples_to_features\n",
    "\n",
    "将InputFeature保存为TFrecords 数据格式\n",
    "\n",
    "https://blog.csdn.net/weixin_42001089/article/details/90236241"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_based_convert_examples_to_features(\n",
    "        examples, label_list, max_seq_length, tokenizer, output_file):\n",
    "    \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
    "\n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        feature = convert_single_example(ex_index, example, label_list,\n",
    "                                         max_seq_length, tokenizer)\n",
    "\n",
    "        def create_int_feature(values):\n",
    "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "            return f\n",
    "\n",
    "        features = collections.OrderedDict()\n",
    "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "        features[\"label_ids\"] = create_int_feature(feature.label_id)  # 修改 : 把中括号去掉了\n",
    "        features[\"is_real_example\"] = create_int_feature(\n",
    "                [int(feature.is_real_example)])\n",
    "\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_int_feature(values):\n",
    "    f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "    return f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = f\n",
    "features = collections.OrderedDict()\n",
    "features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "features[\"label_ids\"] = create_int_feature(feature.label_id)  # 修改 : 把中括号去掉了\n",
    "features[\"is_real_example\"] = create_int_feature(\n",
    "        [int(feature.is_real_example)])\n",
    "\n",
    "tf_example = tf.train.Example(features=tf.train.Features(feature=features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_file = 'bert/tmp/dev.tf_record'\n",
    "examples = processer.get_dev_examples(data_dir)\n",
    "label_list = processer.get_labels()\n",
    "max_seq_length = 128\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=False)\n",
    "# output_file = dev_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_based_convert_examples_to_features(\n",
    "                examples, label_list, max_seq_length, tokenizer, dev_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1]",
   "language": "python",
   "name": "conda-env-tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "226.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
